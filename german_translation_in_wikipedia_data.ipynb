{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30733,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "German translation in wikipedia data",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aleenakjames/German-Translation-using-LLMs/blob/main/German_translation_in_wikipedia_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing all the required libraries**\n"
      ],
      "metadata": {
        "id": "pB-IH7A1VUr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community nltk==3.5 sacrebleu sacremoses"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-06-13T06:08:29.215719Z",
          "iopub.execute_input": "2024-06-13T06:08:29.216168Z",
          "iopub.status.idle": "2024-06-13T06:09:01.443772Z",
          "shell.execute_reply.started": "2024-06-13T06:08:29.216138Z",
          "shell.execute_reply": "2024-06-13T06:09:01.441766Z"
        },
        "trusted": true,
        "id": "HMXuVzcNVUr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Libraries and Imports**\n",
        "* WebBaseLoader from langchain_community.document_loaders is used for fetching and processing web documents. It is particularly used for retrieveing the wikipedia article being searched for.\n",
        "* RecursiveCharacterTextSplitter splits long texts into smaller, manageable chunks based on character count, ensuring the chunks fit within token limits for processing by language models.\n",
        "* BLEU score,Translation Edit Rate (TER), and ChrF score are the metrices for the evaluation of the translations."
      ],
      "metadata": {
        "id": "gLxAvPeEVUr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, MarianMTModel, MarianTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from datasets import load_metric\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-13T07:14:46.368922Z",
          "iopub.execute_input": "2024-06-13T07:14:46.369478Z",
          "iopub.status.idle": "2024-06-13T07:14:47.570588Z",
          "shell.execute_reply.started": "2024-06-13T07:14:46.369436Z",
          "shell.execute_reply": "2024-06-13T07:14:47.569258Z"
        },
        "trusted": true,
        "id": "xhlvCB4oVUsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading data from wikipedia**\n",
        "The WebBaseLoader is initialized with the URL of the Wikipedia page for Koalas in German (\"https://de.wikipedia.org/wiki/koala\"). This sets up the loader to fetch and process the content from this URL."
      ],
      "metadata": {
        "id": "zjkr0XxNVUsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the required url\n",
        "loader = WebBaseLoader(\"https://de.wikipedia.org/wiki/Koala\")\n",
        "#load the data\n",
        "data = loader.load()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-13T07:52:03.539937Z",
          "iopub.execute_input": "2024-06-13T07:52:03.54047Z",
          "iopub.status.idle": "2024-06-13T07:52:03.858539Z",
          "shell.execute_reply.started": "2024-06-13T07:52:03.540431Z",
          "shell.execute_reply": "2024-06-13T07:52:03.856988Z"
        },
        "trusted": true,
        "id": "RW31cyKaVUsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**\n",
        "Unwanted characters such as newlines, pipes, slashes, parentheses, square brackets, and backslashes are removed from each chunk.The beginning and end of each chunk are trimmed to remove extraneous content. This preprocessing step ensures that the text chunks are cleaned and ready for further processing or analysis.\n",
        "\n",
        "**Splitting large texts**\n",
        "\n",
        "split_documents() method of the text_splitter instance to split the loaded data into smaller chunks. The data variable, which contains the text loaded from the web page, is divided into chunks based on the specified chunk_size (Each chunk will have a maximum of 2000 characters) and chunk_overlap (There will be an overlap of 200 characters between consecutive chunks. This ensures that important information at the boundaries of chunks is not lost.) The resulting chunks are stored in the docs variable."
      ],
      "metadata": {
        "id": "u1lUHW4iVUsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split the data into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100, )\n",
        "docs= text_splitter.split_documents(data)"
      ],
      "metadata": {
        "_kg_hide-output": false,
        "execution": {
          "iopub.status.busy": "2024-06-13T07:52:07.50855Z",
          "iopub.execute_input": "2024-06-13T07:52:07.509035Z",
          "iopub.status.idle": "2024-06-13T07:52:07.519448Z",
          "shell.execute_reply.started": "2024-06-13T07:52:07.509001Z",
          "shell.execute_reply": "2024-06-13T07:52:07.517387Z"
        },
        "trusted": true,
        "id": "h7RvvILxVUsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a translation table that maps each character to None\n",
        "remove_chars = \"|\\\\/()[]\"\n",
        "translation_table = str.maketrans('', '', remove_chars)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-13T07:52:19.329674Z",
          "iopub.execute_input": "2024-06-13T07:52:19.330196Z",
          "iopub.status.idle": "2024-06-13T07:52:19.337454Z",
          "shell.execute_reply.started": "2024-06-13T07:52:19.330158Z",
          "shell.execute_reply": "2024-06-13T07:52:19.335649Z"
        },
        "trusted": true,
        "id": "C5-jKMdlVUsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove unwanted characters in each chunk of data\n",
        "for i in range(0, len(docs)):\n",
        "  if docs[i] is not None:\n",
        "    docs[i] = str(docs[i]).replace(\"\\\\n\", \"\").translate(translation_table)\n",
        "    docs[i] = docs[i][22:-104]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-13T07:52:21.669406Z",
          "iopub.execute_input": "2024-06-13T07:52:21.669836Z",
          "iopub.status.idle": "2024-06-13T07:52:21.684375Z",
          "shell.execute_reply.started": "2024-06-13T07:52:21.669804Z",
          "shell.execute_reply": "2024-06-13T07:52:21.682734Z"
        },
        "trusted": true,
        "id": "lPSvCKFZVUsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference using different models**\n",
        "* **mBART:** The model can translate directly between any pair of 50 languages. To translate into a target language, the target language id is forced as the first generated token. To force the target language id as the first generated token, pass the forced_bos_token_id parameter to the generate method.\n",
        "* **MarianMT:** The model \"Helsinki-NLP/opus-mt-de-en\" is a neural machine translation model designed specifically for translating text from German (de) to English (en), and it is part of the OPUS-MT project developed by the Helsinki-NLP group.\n",
        "* **Tsmall100:** It is a compact and fast massively multilingual machine translation model covering more than 10K language pairs, that achieves competitive results while being much smaller and faster."
      ],
      "metadata": {
        "id": "NUCCwcv6VUsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Models and Tokenizers\n",
        "models = {\n",
        "    \"MarianMT\": (\"Helsinki-NLP/opus-mt-de-en\", MarianMTModel, MarianTokenizer),\n",
        "    \"mBART\": (\"facebook/mbart-large-50-many-to-many-mmt\", MBartForConditionalGeneration, MBart50TokenizerFast),\n",
        "    \"Tsmall100\": (\"alirezamsh/small100\", AutoModelForSeq2SeqLM, AutoTokenizer)\n",
        "}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-13T06:12:24.220368Z",
          "iopub.execute_input": "2024-06-13T06:12:24.220822Z",
          "iopub.status.idle": "2024-06-13T06:12:24.228244Z",
          "shell.execute_reply.started": "2024-06-13T06:12:24.220789Z",
          "shell.execute_reply": "2024-06-13T06:12:24.226343Z"
        },
        "trusted": true,
        "id": "8e46NcUXVUsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({})\n",
        "inferenceTime = {}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-13T06:12:27.123955Z",
          "iopub.execute_input": "2024-06-13T06:12:27.124451Z",
          "iopub.status.idle": "2024-06-13T06:12:27.131367Z",
          "shell.execute_reply.started": "2024-06-13T06:12:27.124415Z",
          "shell.execute_reply": "2024-06-13T06:12:27.129419Z"
        },
        "trusted": true,
        "id": "KmwLF9JAVUsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, (model_checkpoint, model_class, tokenizer_class) in models.items():\n",
        "    # Load model and tokenizer\n",
        "    print(model_name)\n",
        "    model = model_class.from_pretrained(model_checkpoint)\n",
        "    tokenizer = tokenizer_class.from_pretrained(model_checkpoint)\n",
        "    translated = []\n",
        "    #translate each chunk to english\n",
        "    start_time = time.time()\n",
        "    for i in range(0, len(docs)):\n",
        "        if model_name == \"mBART\":\n",
        "            tokenizer.src_lang = \"de_DE\"\n",
        "            encoded_de = tokenizer(docs[i], return_tensors=\"pt\")\n",
        "            generated_tokens = model.generate(\n",
        "                **encoded_de,\n",
        "                forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"]\n",
        "            )\n",
        "            translated.append(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n",
        "        elif model_name == \"Tsmall100\":\n",
        "            encoded_de = tokenizer(docs[i], return_tensors=\"pt\")\n",
        "            generated_tokens = model.generate(**encoded_de)\n",
        "            translated.append(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n",
        "        else:\n",
        "            encoded_de = tokenizer(docs[i], return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "            generated_tokens = model.generate(**encoded_de)\n",
        "            translated.append(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n",
        "    translated_text = [element[0] for element in translated]\n",
        "    end_time = time.time()\n",
        "    inference_time = end_time - start_time\n",
        "    inferenceTime[model_name] = inference_time\n",
        "    df[model_name] = translated_text\n",
        "    df.to_csv(\"translation.csv\", index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-13T06:12:28.724635Z",
          "iopub.execute_input": "2024-06-13T06:12:28.725152Z",
          "iopub.status.idle": "2024-06-13T07:14:10.740539Z",
          "shell.execute_reply.started": "2024-06-13T06:12:28.725116Z",
          "shell.execute_reply": "2024-06-13T07:14:10.738201Z"
        },
        "trusted": true,
        "id": "WPnortKgVUsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Calculating Scores**\n",
        "**BLEU:** The score evaluates the quality of machine-generated translations by comparing them to reference translations. Weights define the importance of different n-grams in the BLEU score calculation.\n",
        "\n",
        "**TER:** The Translation Edit Rate score measures the number of edits required to change a machine-translated output into one of the reference translations, with a lower score indicating a higher quality translation.\n",
        "\n",
        "**ChrF:** The Character F-score metric is another evaluation metric for machine translation quality, which computes precision and recall over character n-grams, not word n-grams."
      ],
      "metadata": {
        "id": "lHFFnVDuVUsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_metric = load_metric('sacrebleu', trust_remote_code=True)\n",
        "ter_metric = load_metric('ter', trust_remote_code=True)\n",
        "chrf_metric = load_metric('chrf', trust_remote_code=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-13T07:15:32.877473Z",
          "iopub.execute_input": "2024-06-13T07:15:32.878059Z",
          "iopub.status.idle": "2024-06-13T07:15:33.997523Z",
          "shell.execute_reply.started": "2024-06-13T07:15:32.878017Z",
          "shell.execute_reply": "2024-06-13T07:15:33.99631Z"
        },
        "trusted": true,
        "id": "QAaCnLJKVUsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions to evaluate translations\n",
        "def evaluate_bleu(predictions, references):\n",
        "    return bleu_metric.compute(predictions=predictions, references=[[ref] for ref in references])\n",
        "def evaluate_chrf(predictions, references):\n",
        "    return chrf_metric.compute(predictions=[pred.split() for pred in predictions], references=[[ref.split()] for ref in references])\n",
        "def evaluate_ter(predictions, references):\n",
        "    return ter_metric.compute(predictions=[pred.split() for pred in predictions], references=[[ref.split()] for ref in references])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-13T07:16:02.447341Z",
          "iopub.execute_input": "2024-06-13T07:16:02.447833Z",
          "iopub.status.idle": "2024-06-13T07:16:02.458139Z",
          "shell.execute_reply.started": "2024-06-13T07:16:02.447794Z",
          "shell.execute_reply": "2024-06-13T07:16:02.456249Z"
        },
        "trusted": true,
        "id": "IEsKn49ZVUsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "keys = list(models.keys())\n",
        "\n",
        "# Calculate all pairwise comparisons\n",
        "for i in range(len(keys)):\n",
        "    for j in range(len(keys)):\n",
        "        if i != j:\n",
        "            bleu = evaluate_bleu(df[keys[i]], df[keys[j]])\n",
        "            ter = evaluate_ter(df[keys[i]], df[keys[j]])\n",
        "            chrf = evaluate_chrf(df[keys[i]], df[keys[j]])\n",
        "            results[(keys[i], keys[j])] = {\n",
        "                \"BLEU\": bleu,\n",
        "                \"TER\": ter,\n",
        "                \"ChrF\": chrf\n",
        "            }"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-13T07:16:24.144776Z",
          "iopub.execute_input": "2024-06-13T07:16:24.145295Z",
          "iopub.status.idle": "2024-06-13T07:21:38.666628Z",
          "shell.execute_reply.started": "2024-06-13T07:16:24.145261Z",
          "shell.execute_reply": "2024-06-13T07:21:38.665041Z"
        },
        "trusted": true,
        "id": "zMH-F7WoVUsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model, time_s in inferenceTime.items():\n",
        "        print(f\"Inferece time of {model}: {time_s}\")\n",
        "for (model1, model2), result in results.items():\n",
        "    print(f\"Model: {model1} to {model2}\")\n",
        "    for metric, score in result.items():\n",
        "        print(f\"  {metric}: {score}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-13T07:25:15.414475Z",
          "iopub.execute_input": "2024-06-13T07:25:15.415064Z",
          "iopub.status.idle": "2024-06-13T07:25:15.424417Z",
          "shell.execute_reply.started": "2024-06-13T07:25:15.415018Z",
          "shell.execute_reply": "2024-06-13T07:25:15.422491Z"
        },
        "trusted": true,
        "id": "guCI9AwsVUsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualising the results**"
      ],
      "metadata": {
        "id": "jH4GdSrFVUsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize dictionaries to hold DataFrames for each metric\n",
        "metrics = ['BLEU', 'TER', 'ChrF']\n",
        "metric_dfs = {metric: pd.DataFrame(index=keys, columns=keys) for metric in metrics}\n",
        "\n",
        "# Populate the DataFrames with the 'score' from each metric's dictionary\n",
        "for (model1, model2), scores in results.items():\n",
        "    for metric in metrics:\n",
        "        metric_dfs[metric].at[model1, model2] = scores[metric]['score']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-13T07:47:15.070719Z",
          "iopub.execute_input": "2024-06-13T07:47:15.071647Z",
          "iopub.status.idle": "2024-06-13T07:47:15.087052Z",
          "shell.execute_reply.started": "2024-06-13T07:47:15.071589Z",
          "shell.execute_reply": "2024-06-13T07:47:15.085368Z"
        },
        "trusted": true,
        "id": "X-yRtJx9VUsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_heatmap(df, title):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(df.astype(float), annot=True, cmap='coolwarm', fmt='.2f')\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-13T07:47:27.907108Z",
          "iopub.execute_input": "2024-06-13T07:47:27.907607Z",
          "iopub.status.idle": "2024-06-13T07:47:27.915435Z",
          "shell.execute_reply.started": "2024-06-13T07:47:27.907572Z",
          "shell.execute_reply": "2024-06-13T07:47:27.913697Z"
        },
        "trusted": true,
        "id": "wgy-exiiVUsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the results for each metric\n",
        "for metric in metrics:\n",
        "    plot_heatmap(metric_dfs[metric], f'{metric} Scores')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-13T07:47:41.456117Z",
          "iopub.execute_input": "2024-06-13T07:47:41.456583Z",
          "iopub.status.idle": "2024-06-13T07:47:42.581134Z",
          "shell.execute_reply.started": "2024-06-13T07:47:41.456548Z",
          "shell.execute_reply": "2024-06-13T07:47:42.579526Z"
        },
        "trusted": true,
        "id": "h66OMQ8MVUsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O6Xy3DK_VUsG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
